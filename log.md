# 100 Days Of Code - Log

### Day 1: June 14, 2022

**Today's Progress**:
Learned the fundamentals concepts of machine learning: what machine learning is, and the 2 main types (supervises and unsupervised learning) along with the examples of regression, classification, and clustering.

**Thoughts**:
Machine learning is very interesting, the idea of computers progressing through an experience to predict a future result is fascinating. Looking forward in applying the maths I've learned in school and uni.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University

### Day 2: June 15, 2022

**Today's Progress**:
Advancing on linear regression for machine learning: learned model representation, cost function, and gradient descent for linear regression. Reviewed linear algebra regarding matrices and vectors along with their operations and properties.

**Thoughts**:
Years of calculus and linear algebra are now proving to be useful, can't wait for learning how to truly apply these theories to computers.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University

### Day 3: June 16, 2022

**Today's Progress**:
Learned new regression method of normal equation, its advantages and disadvantages compared to gradient descent, and how to overcome its noninvertibility problem.

**Thoughts**:
The comparison between gradient descent and normal equation is pretty interesting.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University

### Day 4: June 17, 2022

**Today's Progress**:
Implemented cost function & gradient descent in Python (using the assignment from Machine Learning course by Andrew Ng, converted to Python). Hours of trying to apply the equation to the code, and turns out vectorization helps a lot.

**Thoughts**:
Matrices are very tricky, and putting equation involving matrices into the code can be challenging to understand.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University

### Day 5: June 18, 2022

**Today's Progress**:
Implemented cost function, gradient descent, feature scaling, and normal equation for multi-variable linear regression (finished the course assignment). Learned the fundamentals of classification using logistic regression and decision boundary.

**Thoughts**:
Getting the hang of using matrices, but feature scaling part was really tricky. Still learning about the basics of classification, can't wait to learn more.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University

### Day 6: June 19, 2022

**Today's Progress**:
Advancing on logistic regression: learned the cost function, gradient descent, more advanced optimizations, namely conjugate gradient, BFGS, and L-BFGS (but not discussed in detail, would take weeks), and multiclass classification.

**Thoughts**:
Interesting to see the differences between logistic regression model and linear regression model.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University

### Day 7: June 20, 2022

**Today's Progress**:
Finished the assignment on implementing sigmoid function, logistic regression cost & regression (also using regularization) in Python. Learned the concepts of neural networks along with vectorization and also some applications of neural networks.

**Thoughts**:
Neural network has a mind-blowing concept, imitating the work of neurons to handle complex functions. Can't wait to dive deeper into the applications of neural networks.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University

### Day 8: June 21, 2022

**Today's Progress**:
Had to skip neural networks today to finish writing 3 articles (in Indonesian) on Medium (my profile: https://bit.ly/3n6OjbQ) about SQL, Google Data Studio, and Python for the Data Science Bootcamp provided by MySkill (https://bit.ly/3yaCzvo).

**Thoughts**:
Writing 3 articles is a LOT. However, explaining things to other people is always very helpful in improving my understanding of the materials.

**Learning Platform**:
Coursera, Data Science Bootcamp by MySkill, and Medium

### Day 9: June 22, 2022

**Today's Progress**:
Finished making the algorithm for training multiple regularized logistic regression classifiers to detect handwritten digits (as an assignment), but still needed a lot of help. Definitely gonna review multiclass classification with neural networks.

**Thoughts**:
Implementing multiple regularized logistic regression is quite challenging.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University

### Day 10: June 23, 2022

**Today's Progress**:
Finished the assignment on making one-vs-all classifier and neural network prediction for detecting handwritten digits. Also implemented queueing a whole YT playlist and added queue pagination for my discord music bot.

**Thoughts**:
One-vs-all classifier felt difficult, and still need to review it later on. Developing the Discord music bot side project felt super fun.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University


### Day 11: June 24, 2022

**Today's Progress**:
Learned about the cost function for neural networks, backpropagation algorithm to train the weights used by forward propagation. Too much to take, still can't make much sense of my notes, probably have to take it slower and review a few times.

**Thoughts**:
Neural network is a whole new concept which is as intriguing as it is challenging, especially backpropagation, need some time to digest everything.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University


### Day 12: June 25, 2022

**Today's Progress**:
Learned how to unroll parameters (theta) to pass to minimization function as a vector, then compute cost function and its derivative using the unrolled parameters. Also added shuffle command to my Discord music bot.

**Thoughts**:
Unrolling parameters itself is trivial, the hard part is mentally understanding it as a vector and bringing it into operation with other matrices. Again, developing my Discord music bot was fun.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University


### Day 13: June 26, 2022

**Today's Progress**:
Learned gradient checking (comparing gradient values with numerical approximation), random weights initialization, and steps on training a neural network (initialization, forward prop, compute cost, backprop, gradient checking, and optimization).

**Thoughts**:
Understanding the thought process behind the neural network was challenging, and looking forward to the adversities of implementing it in Python.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University


### Day 14: June 27, 2022

**Today's Progress**:
Finished an assignment on implementing neural networks into Python code, including forward propagation and back propagation (with regularized cost function and gradients). Reviewing everything again, especially on backprop.

**Thoughts**:
Hahahaha I thought I knew everything. After implementing NN in Python, I realized I knew nothing and looked for help a lot, definitely gonna take time to comprehend everything.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University

### Day 15: June 28, 2022

**Today's Progress**:
Still reviewing on neural network, especially on backpropagation. Other resources I'm using to review is from 3Blue1Brown and Michael Nielsen's online book about Neural networks and deep learning.

**Thoughts**:
After taking a new approach by reviewing with the help of 3Blue1Brown and M. Nielsen's book, I'm starting to get the hang of it, at least the concepts became much clearer, hope everything else follows along.

**Learning Platform**:
3Blue1Brown YT Channel and M. Nielsen's Online Book on NN and Deep Learning

### Day 16: June 29, 2022

**Today's Progress**:
(Still) reviewing on Neural Network implementation in Python with the help of M. Nielsen's online book. The book is very helpful and provides a code which I can implement myself with Google Colab https://bit.ly/3AeHZGR. Now onto backpropagation ðŸ’ª

**Thoughts**:
The book was VERY helpful. Concepts are explained clearly with a readable code that I can practice on myself. Hoping everything I understand sticks with me as I learn newer things.

**Learning Platform**:
M. Nielsen's Online Book on NN and Deep Learning


### Day 17: June 30, 2022

**Today's Progress**:
Reviewed the heart of backpropagation in neural network: partial derivatives and chain rule. The maths behind backprop is amazing, but I still need to work on implementing it in Python (especially using vectorization).

**Thoughts**:
The maths behind backprop is beautiful, and I got to implement everything I know about multivariate Calculus I learned back then. Implementing in Python is a whole other challenge, especially due to the mental challenge that goes into ipmlementing vectorization.

**Learning Platform**:
M. Nielsen's Online Book on NN and Deep Learning

### Day 18: July 1, 2022

**Today's Progress**:
Finished reviewing backpropagation and finally got a good grip on implementing it in Python. Some little parts still feel a bit abstract, so I'll come back occasionally. Also joined Using Python to Interact with OS course on Coursera x Google.

**Thoughts**:
Managing to get a good grip on implementing backprop itself is a huge progress for me (though I still have to review occasionally). I'm also really looking forward to another course I joined on Coursera (Using Python to Interact with OS course) as the second course if IT Automation with Python specialization provided by Google.

**Learning Platform**:
M. Nielsen's Online Book on NN and Deep Learning, Coursera: Using Python to Interact with OS course by Google

### Day 19: July 2, 2022

**Today's Progress**:
ML course: learned about ways to troubleshoot errors in prediction and evaluating a hypothesis by splitting data into training and test sets, then finding the test set error. Automation course: learned about Qwiklabs and using nano on Linux.

**Thoughts**:
So far the steps for troubleshooting and evaluating errors in ML seem straightforward, but I'm expecting rougher challenges ahead. Simulating Linux using Qwiklabs and using nano on Linux terminal, on the other hand, was seriously fun.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University, Coursera: Using Python to Interact with OS course by Google
