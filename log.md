# 100 Days Of Code - Log

### Day 1: June 14, 2022

**Today's Progress**:
Learned the fundamentals concepts of machine learning: what machine learning is, and the 2 main types (supervises and unsupervised learning) along with the examples of regression, classification, and clustering.

**Thoughts**:
Machine learning is very interesting, the idea of computers progressing through an experience to predict a future result is fascinating. Looking forward in applying the maths I've learned in school and uni.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University

### Day 2: June 15, 2022

**Today's Progress**:
Advancing on linear regression for machine learning: learned model representation, cost function, and gradient descent for linear regression. Reviewed linear algebra regarding matrices and vectors along with their operations and properties.

**Thoughts**:
Years of calculus and linear algebra are now proving to be useful, can't wait for learning how to truly apply these theories to computers.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University

### Day 3: June 16, 2022

**Today's Progress**:
Learned new regression method of normal equation, its advantages and disadvantages compared to gradient descent, and how to overcome its noninvertibility problem.

**Thoughts**:
The comparison between gradient descent and normal equation is pretty interesting.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University

### Day 4: June 17, 2022

**Today's Progress**:
Implemented cost function & gradient descent in Python (using the assignment from Machine Learning course by Andrew Ng, converted to Python). Hours of trying to apply the equation to the code, and turns out vectorization helps a lot.

**Thoughts**:
Matrices are very tricky, and putting equation involving matrices into the code can be challenging to understand.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University

### Day 5: June 18, 2022

**Today's Progress**:
Implemented cost function, gradient descent, feature scaling, and normal equation for multi-variable linear regression (finished the course assignment). Learned the fundamentals of classification using logistic regression and decision boundary.

**Thoughts**:
Getting the hang of using matrices, but feature scaling part was really tricky. Still learning about the basics of classification, can't wait to learn more.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University

### Day 6: June 19, 2022

**Today's Progress**:
Advancing on logistic regression: learned the cost function, gradient descent, more advanced optimizations, namely conjugate gradient, BFGS, and L-BFGS (but not discussed in detail, would take weeks), and multiclass classification.

**Thoughts**:
Interesting to see the differences between logistic regression model and linear regression model.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University

### Day 7: June 20, 2022

**Today's Progress**:
Finished the assignment on implementing sigmoid function, logistic regression cost & regression (also using regularization) in Python. Learned the concepts of neural networks along with vectorization and also some applications of neural networks.

**Thoughts**:
Neural network has a mind-blowing concept, imitating the work of neurons to handle complex functions. Can't wait to dive deeper into the applications of neural networks.

**Learning Platform**:
Coursera: Machine Learning course by Stanford University
